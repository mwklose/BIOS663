ID,author,Topic,Question,Ordered,Correct,Ans1,Msg1,Ans2,Msg2,Ans3,Msg3,Ans4,Msg4,Ans5,Msg5
poly1,mwk+tim,poly,What is the maximum number order that can be fit in Polynomial regression?,FALSE,$n - 1$,$n$,"Remember that two points make a line, three points define a parabola, and so forth",$n - 1$,Woohoo!,$p$,We have not introduced the concept of predictors in this question,$p - 1$,We have not introduced the concept of predictors in this question,$1$,"Parabolas exist, as do cubic functions, etc"
poly2,mwk+tim,poly,Which of the following is true about orthogonal polynomials?,FALSE,They are linear combinations of original data.;They are mean zero (except for the intercept);They are mutually ortogonal (uncorrelated),They are linear combinations of original data.,Woohoo!,They do not contain the same information as the original variables.,Orthogonal coding works because it DOES incorporate the same information,The are mean zero (including the intercept),Not including the intercept :),They are mean zero (except for the intercept),Intercept does not have to be zero!!!,They are mutually ortogonal (uncorrelated),This is true!
poly3,mwk+tim,poly,"In orthogonal polynomial coding, what does each variable represent?",FALSE,"$z_1$ captures all information in $x$ beyond the intercept, $z_2$ captures all information in $x^2$ beyond the linear term and intercept, and so forth","$z_1$ captures all information in $x$ beyond the intercept, $z_2$ captures all information in $x^2$ beyond the intercept, and so forth",Reread this carefully :),"$z_1$ captures all information in $x$ beyond the intercept, $z_2$ captures all information in $x^2$ beyond the linear term and intercept, and so forth",Correct!,"$z_1$ captures all information in $x_1$ beyond the intercept, $z_2$ captures all information in $x_2$ beyond the intercept, and so forth","These are subscripts, which then makes this traditional regression","$z_1, z_2$ are completely unrelated to input matrix $x$.",This is wrong. Ooof.,None of the above,There are some of the above...
poly4,kelly,poly,"You have one predictor, one outcome, and 6 data points. What is the minimum order of polynomial would perfectly fit this data, and is it appropriate to use that order in your model?",FALSE,"5, but it would be inappropriate unless you can justify and interpret the use of the higher order polynomial","6, and it would be appropriate because the model will hit all 6 data points with no residuals","It only takes 5 orders to hit all 6 points (remember that a parabola from a second order polynomial can hit three points, and a perfect fit of the model doesn't mean it represents the actual relationship well)","6, but it would be inappropriate unless you can justify and interpret the use of the higher order polynomial",We do not need a 6th order polynomial to perfectly model 6 data points,"5, and it would be appropriate because the model will hit all 6 data points with no residuals",a perfect fit of the model doesn't mean it represents the actual relationship well. How would you interpret a 5th order polynomial for a simple predictor-outcome relationship?,"5, but it would be inappropriate unless you can justify and interpret the use of the higher order polynomial",Heck yeah babyyyyy,None of the above,There are some of the above...
poly5,kelly,poly,"when adding additional orders to a natural polynomial, which of the following is a concern for your model fit?",FALSE,inflation of variance due to collinearity between terms that are based on the same input predictor,inflation of variance due to collinearity between terms that are based on the same input predictor,"Yep! If two terms in the model are derived from the same input data, they will liekly have high collinearity and variance inflation",Inability to capture nonlinear relationships between a predictor and outcome,The purpose of adding additional orders to a polynomial is to allow our model to deviate from a linear relationshio,Adding orders to a polynomial are hard to justify or interpret,"As a rule, adding anything above a cubic polynomial needs to be justified and can be hard to interpret, but lower orders can be added to represent a nonlinear relationship between a predictor and the outcome",All of the above,Incorrect,None of the above,There are some of the above...
poly6,kelly,poly,"in an orthogonal polynomial model with coefficients estimates $a_0$ to $a_k$, performing an F test on paramter $a_3$ is equivalent to what in a natural polynomial model with coefficient estimates $\beta_0$ to $\beta_k$?",FALSE,An in-order test for $\beta_3$,An added-last test for $\beta_3$,Orthogonal polynomials contain only the information added by the additional order of the polynomial and nothing after. An added last test would also be testing against the higher order polynomials in the model in addition to the earlier ones,An in-order test for $\beta_3$,"Slay queen! orthogonal polynomials should contain all additional information added by that order of the natura polynomial, none of the earlier redundant information, and none of the information that comes after",It depends on whether the F test of $a_3$ is added in-order or added-last,"orthogonal polynomials have no collinearity between the input data for each term, so in-order and added-last tests will be equivalent","Nothing, as they are different models","Orthogonal and natural polynomial models are made from the same data, just arranged differently",None of the above,There are some of the above...
poly7,kelly,poly,"After graphing a predictor against an outcome, you see the relationship between the two is linear (and not flat) but changes directions starkly at 4 predictor thresholds. What is an appropriate response?",FALSE,Set linear splines at the thresholds,"Fit a normal linear regression, averaging the relationship across the distribution","This could be justified, but the final regression won't reflect the changing relationships across the predictor distribution",Set linear splines at the thresholds,Yes! These splines will allow the linear relationship to change at these thresholds,Add multiple orders of natural polynomials,"This could work in some cases, but would make the line deviate from linearity between the thresholds. Also, fitting the breakpoints with natural polynomials could require higher than 3 or 4 orders, which is hard to interpret",Give up,"Slay queen! The world is too complex to be meaningfully modeled and your very existence is just a blip in the background chemical chaos of the universe! This answer is still technically incorrect, but don't let that stop you!",None of the above,There are some of the above...
poly8,kelly,poly,Which of the following is NOT true regarding moving average and loess regression,FALSE,They work well both in the middle of the data distribution and at the edges,"For a given $x$, the input values for a moving average are weighted equally, while loess gives greater weight to input values with a predictor value closer to x","Actually, this statement is true! While moving average and loess can include different amounts of the total data when trying to calculate a predicted $Y$ for a given $x$, loess typically gives greater weight to input values whose $x$-value is closer to the desired $x$",They work well both in the middle of the data distribution and at the edges,"Correct; this is not true! Both methods involve predicting Y for a given x by averaging the Y values for data points with x's close to the given x. At the edges of the distirbution, however, there are no data points further out to include in the average, so the predicted Y is unduly influenced by the data points that ARE present closer to the center of the distribution","Both are nonparametric regressions, allowing for nonlinear and distribution specific prediction of $Y$","This is actually true! Both have reduced power to detect monotonic or linear relationships relative to regular parametric regression (like linear regression), but are better able to represent complex or nonlinear relationships",They are typically applied for predictors and outcomes that are both continuous,"This is actually true! Both methods work by averaging the Y values for data points whose x values are close to the given x we want to predict. This requires that the Y values can be averaged (so must be continuous) and that x-values can be defined as ""close to each other"", which typically means continuous (although can apply for certain ordinal categories but not usually)",None of the above,There are some of the above...
poly9,qoua,poly,"You have 1000 of x and y pairs, you create an essence matrix that is 92 x 1. What order polynomial will past through all the points with a $\sigma^2 = 0$?",FALSE,91,1000,Some points are redundant,999,The essence matrix shows what points are redundant,92,"It takes 2 points to fit a straight line, 3 points to fit a parabola",91,Correct,90,One more point is needed
poly10,mwk,poly,A moving average nonparametric smoother,TRUE,Is problematic at the upper and lower limits of $X$,Is problematic at the upper and lower limits of $X$,"This is true, but is it the only thing that is true",Is not sensitive to interval width,It is sensitive to interval width - depends if you use 3 or 5 or 7 or etc.,Work best with one categorical predictor and one continuous predictor,"Lecture 11, Slide 38",Both I and II,II is not true,Both I and III,III is not true
poly11,mwk,poly,The superior nonparametric smoother is,FALSE,"Loess, because it is gives most weight to points close to $X=x$, i.e. locally weighted least squares","Moving average, because of its computational simplicity",Moving average has issues at boundaries,"Loess, because it minimizes finds the local curve with minimized $\beta$ values","I made this up, it focuses on weighted least squares","Loess, because it is the least squares fit for all values within a suitable interval","I mean, this could be true but there is a better answer","Loess, because it is gives most weight to points close to $X=x$, i.e. locally weighted least squares",This is correct.,"Loess, for a combination of two reasons above","Not really, but I understand if this was confusing"
transform1,kelly,transform,"You've applied a $log(y)$ transformation to the outcome and found a beta coefficient. If you increase x by one, what will the new predicted $y$ be? $y_0$ = previous predicted $y$",FALSE,$y_0*e^{\beta}$,$y_0 * 100e^{\beta - 1}$,"You're close! The $100e^{\beta} - 1$ is used to give the percent increase in $y$, so if $\beta= 0.405$ then $100 (e^{0.405} -1) = 50$, representing a 50% increase! However, $y_0*50$ is 50 times the $y_0$ value, not a 50% percent increase",$y_0*\beta$,"This beta was calculated based on $log(y)$ so, to find its meaning in terms of y, you have to exponentiate the beta just like you exponentiate the y",$y_0*e^{\beta}$,"that's right! $\beta$ was calculated using the $log(y)$ so, to apply it to y on the non-log scale, you have to exponentiate $\beta$ just like you exponentiate $y$",$y_0 + \beta$,"This would work on a linear scale, but beta was calculated to reduce SSE on the LOG scale of y, so you can't apply this beta directly back to the untransformed y",None of the above,There are some of the above...
transform2,kelly,transform,What is the value of a Box-Cox transformation over a plain power transformation?,FALSE,The Box-Cox transformation puts the SSEs on the same scale for easy comparison when trying multiple different powers,The Box-Cox transformation puts the SSEs on the same scale for easy comparison when trying multiple different powers,"That's right! If we want to find the best power transformation for an outcome, plain power transformations will cause the scale of the SSE to change between different powers, making it ihard to compare. Box-cox can tests different powers while keeping SSE on the same scale, so you can compare which of the tested powers minimized the SSE",They are equivalent,"Um, the equations aren't the same, and box-cox is there for a reason. bitch.","A box-cox power transformation won't tolerate a power transformation to the power of 0, as all $y$'s will become 1","This is actually a weakness of the plain power transformation. In box-cox, if the power tested is 0, then we employ a $log(y)$ transformation instead of the regular equation",All of the above,Incorrect,None of the above,There are some of the above...
select1,kelly,select,"You are comparing candidate models to a null model. Of the following 4 metrics for comparing two models, which is NOT paired with a correct limitation?",FALSE,F-test: can't test candidates against null models if neither has an intercept,F-test: can't test candidates against null models if neither has an intercept,"Yeah this one is wrong. F-tests can compare any two models as long as one is nested in the other, so the SSE of the larger model is entirely contained within the greater SSE of the smaller model. We often compare a more complete model to the intercept-only model to see if it functions better at explaining the data but, if there is no intercept, then the smaller model can just be the total null model ($y$ is average $y$ value)","$R^2$: always decreases if a predictor is removed, unless using adjusted form","Sorry, hunty, but this is true. The adjusted-$R^2$ uses the same base formula but multiplies the unadjusted-$R^2$ by a ratio that favors a large $n$ but penalizes adding new predictors",SBC: places a larger penalty on large models than AIC,"Sorry but this is a true statement about the SBC. A high AIC or SBC indicates a poor model. Both use the same term to inflate their score if SSE is too high $[n*log(SSE/n)]$ but use different penalty terms relating to adding predictors [p]. AIC adds $2p$ to its score while SBC uses $log(n)p$ and, because $log(n) > 2$ in almost all cases, SBC does penalize addition of predictors more.","Mallows $Cp$: is used to compare a candidate model to a full model, not a null model","This is correct about Mallows $Cp$, which compares the SSE(smaller candidate model) against the MSE(full model) with some other terms relating to n and p, it is not really meaningful if using a null model because it is designed to test the quality of the smaller model, which would be your null model",None of the above,There are some of the above...
select2,kelly,select,Which of the following is an advantage of data-splitting and generating a model based on the training dataset?,FALSE,It reduces risk of overfitting,It maximizes sample size for model creation,teehee you split your data,The model created does not depend on the exact datasplit,Random data-splitting can cause major changes to the generated model by random chance alone,It reduces risk of overfitting,"By generating your model on a smaller subset of your data, you maintain unmodeled data to test your generalizability against",All of the above,Try again,None of the above,There are some of the above...
select3,kelly,select,Which of the following approaches to internal data validation is a combination of the methods and advantages/disadvantages of the other two?,TRUE,$k$-fold cross-validation,Leave-out-one cross-validation,"This is a pretty intense approach, in which SSE is generated by comparing each observation to a predicted value made from a model generated WITHOUT that ONE observation",Single-splitting,"In this course, this refers to splitting the dataset into a training dataset and a test set",$k$-fold cross-validation,"Correct! $k$-fold functions like data-splitting, training on training datasets and comparing to test sets. Howver, you make $k$ partitions in the data and, after jackknifing out the data in that partition and creating a model on everything else, you compare the error of the data in the partition (the test set) to the model generated without the data in the partition (everything else is the training set), sorta like leave-out-one corss-validation but for a group of observations rather than just one",All of the above,Incorrect,None of the above,There are some of the above...
code1,kelly,code,Which of the following is not true about classical ANOVA modeling,FALSE,The mean of group 1 $\mu_1 = \beta_1$,The mean of group 1 $\mu_1 = \beta_1$,"Classical ANOVA modeling uses an unmeasured $\beta_0$ representing the population mean, and then creates $\beta_k$ to be the difference between this $\beta_0$ and the k group mean. $\mu_k = \beta_0 + \beta_1$ and, because $\beta_0$ is chosen arbirtrarily, none of these $\beta$s have intrinsic meaning taken out of context of each other ($\beta_1$ has no meaning by itself), also meaning that the X matrix is not full rank because it has a redundant column.",The beta coefficients are not estimable,"This is acutally true. Classical anova modeling posits that $\mu_k = \beta_0 + \beta_{k}$ and there is one $\beta_k$ for each group as well as a $\beta_0$ to represent the theoretical grand mean of the whole population. However, we don't measure this grand mean, so $\beta_0$ can really be set at any value and all the other $\beta$ coefficients could be adjusted to make sure their addition causes the sum for group k to reach $\mu_k$ and the model would still work. As such, the beta coefficients in classical ANOVA coding do not have inherent interpretability on their own.",The X matrix is not full rank,"This is actually true! The X matrix contains redudnant information in being all 0's and 1's with a unique indicator variable for each group and also an intercept. The other ANOVA models are able to contain the exact same categorical information in an X matrix using one less predictor (such as in the reference cell model, where you have an intercept and k-1 indicator variables and the reference group just has 0's for all the indicators instead of having its own separate variable), which makes them full rank.","The grand mean in a classical ANOVA model is a hypothetical, unmeasured value, whereas the grand mean in effect model coding is the actual mean of the recorded outcome variable across the dataset.",Sorry queen but this is right,,
anova1,kelly,anova,you are testing for homogeneity of variance between 3 treatment groups in your ANOVA. Which is true?,FALSE,A significant bartlett's test indicates heteroscedasticity or just non-normality of the data,Hartley's test examines whether any one group has a different variance from any other,Hartley's checks whether the maximum variance among the groups is significantly different from the minimum variance of the groups,"If homogeneity of variance is rejected, then Welch's ANOVA can't be used",Welch's ANOVA is specifically employed as a version of ANOVA that is still robust when we don't have homogeneity of variance,A significant bartlett's test indicates heteroscedasticity or just non-normality of the data,"Right! A bartlett's test asks tests whether the variance (of errors around each group mean) of at least one group is different from the variance of another. However, the bartlett test can also be significant of one of the groups is not normally distributed",,,,
ancova1,qoua,ancova,ANCOVA requires how many interactions in a model with 2 categorical variables (with 3 levels in each) and 2 continuous variables?,FALSE,0,1,Review Lec 17 Slide 2,2,Review Lec 17 Slide 2,3,Review Lec 17 Slide 2,4,Review Lec 17 Slide 2,None of the above,You are so wrong!
ancova1,qoua,ancova,"How are ANCOVA models different from the ""Full in every cell models?""",FALSE,The slope of the interactions variables betweeen the continous and categorical variable are the same.,The are the same.,Review Lec 17 Slide 2,"ANCOVA model has interactions terms and ""Full model in every cell models"" do not.",Review Lec 17 Slide 2,,Review Lec 17 Slide 2,,Review Lec 17 Slide 2,None of the above,You are so wrong!
code2,qoua,code,"In reference coding, if you have a categorical variable that has 33 levels, how many j columns will you have in your design matrix?",FALSE,33,32,What happen to the intercept?,30,,29,,,,,