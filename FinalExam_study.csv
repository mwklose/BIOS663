ID,author,Question,Ordered,Correct,Ans1,Msg1,Ans2,Msg2,Ans3,Msg3,Ans4,Msg4,Ans5,Msg5
poly1,mwk+tim,What is the maximum number order that can be fit in Polynomial regression?,FALSE,$n - 1$,$n$,"Remember that two points make a line, three points define a parabola, and so forth",$n - 1$,Woohoo!,$p$,We have not introduced the concept of predictors in this question,$p - 1$,We have not introduced the concept of predictors in this question,$1$,"Parabolas exist, as do cubic functions, etc"
poly2,mwk+tim,Which of the following is true about orthogonal polynomials?,FALSE,They are linear combinations of original data.;They are mean zero (except for the intercept);They are mutually ortogonal (uncorrelated),They are linear combinations of original data.,Woohoo!,They do not contain the same information as the original variables.,Orthogonal coding works because it DOES incorporate the same information,The are mean zero (including the intercept),Not including the intercept :),They are mean zero (except for the intercept),Intercept does not have to be zero!!!,They are mutually ortogonal (uncorrelated),This is true!
poly3,mwk+tim,"In orthogonal polynomial coding, what does each variable represent?",FALSE,"$z_1$ captures all information in $x$ beyond the intercept, $z_2$ captures all information in $x^2$ beyond the linear term and intercept, and so forth","$z_1$ captures all information in $x$ beyond the intercept, $z_2$ captures all information in $x^2$ beyond the intercept, and so forth",Reread this carefully :),"$z_1$ captures all information in $x$ beyond the intercept, $z_2$ captures all information in $x^2$ beyond the linear term and intercept, and so forth",Correct!,"$z_1$ captures all information in $x_1$ beyond the intercept, $z_2$ captures all information in $x_2$ beyond the intercept, and so forth","These are subscripts, which then makes this traditional regression","$z_1, z_2$ are completely unrelated to input matrix $x$.",This is wrong. Ooof.,None of the above,There are some of the above...
poly4,kelly,"You have one predictor, one outcome, and 6 data points. What is the minimum order of polynomial would perfectly fit this data, and is it appropriate to use that order in your model?",FALSE,"5, but it would be inappropriate unless you can justify and interpret the use of the higher order polynomial","6, and it would be appropriate because the model will hit all 6 data points with no residuals","It only takes 5 orders to hit all 6 points (remember that a parabola from a second order polynomial can hit three points, and a perfect fit of the model doesn't mean it represents the actual relationship well)","6, but it would be inappropriate unless you can justify and interpret the use of the higher order polynomial",We do not need a 6th order polynomial to perfectly model 6 data points,"5, and it would be appropriate because the model will hit all 6 data points with no residuals",a perfect fit of the model doesn't mean it represents the actual relationship well. How would you interpret a 5th order polynomial for a simple predictor-outcome relationship?,"5, but it would be inappropriate unless you can justify and interpret the use of the higher order polynomial",Heck yeah babyyyyy,None of the above,There are some of the above...
poly5,kelly,"when adding additional orders to a natural polynomial, which of the following is a concern for your model fit?",FALSE,inflation of variance due to collinearity between terms that are based on the same input predictor,inflation of variance due to collinearity between terms that are based on the same input predictor,"Yep! If two terms in the model are derived from the same input data, they will liekly have high collinearity and variance inflation",Inability to capture nonlinear relationships between a predictor and outcome,The purpose of adding additional orders to a polynomial is to allow our model to deviate from a linear relationshio,Adding orders to a polynomial are hard to justify or interpret,"As a rule, adding anything above a cubic polynomial needs to be justified and can be hard to interpret, but lower orders can be added to represent a nonlinear relationship between a predictor and the outcome",All of the above,Incorrect,None of the above,There are some of the above...
poly6,kelly,"in an orthogonal polynomial model with coefficients estimates $a_0$ to $a_k$, performing an F test on paramter $a_3$ is equivalent to what in a natural polynomial model with coefficient estimates $\beta_0$ to $\beta_k$?",FALSE,An in-order test for $\beta_3$,An added-last test for $\beta_3$,Orthogonal polynomials contain only the information added by the additional order of the polynomial and nothing after. An added last test would also be testing against the higher order polynomials in the model in addition to the earlier ones,An in-order test for $\beta_3$,"Slay queen! orthogonal polynomials should contain all additional information added by that order of the natura polynomial, none of the earlier redundant information, and none of the information that comes after",It depends on whether the F test of $a_3$ is added in-order or added-last,"orthogonal polynomials have no collinearity between the input data for each term, so in-order and added-last tests will be equivalent","Nothing, as they are different models","Orthogonal and natural polynomial models are made from the same data, just arranged differently",None of the above,There are some of the above...
poly7,kelly,"After graphing a predictor against an outcome, you see the relationship between the two is linear (and not flat) but changes directions starkly at 4 predictor thresholds. What is an appropriate response?",FALSE,Set linear splines at the thresholds,"Fit a normal linear regression, averaging the relationship across the distribution","This could be justified, but the final regression won't reflect the changing relationships across the predictor distribution",Set linear splines at the thresholds,Yes! These splines will allow the linear relationship to change at these thresholds,Add multiple orders of natural polynomials,"This could work in some cases, but would make the line deviate from linearity between the thresholds. Also, fitting the breakpoints with natural polynomials could require higher than 3 or 4 orders, which is hard to interpret",Give up,"Slay queen! The world is too complex to be meaningfully modeled and your very existence is just a blip in the background chemical chaos of the universe! This answer is still technically incorrect, but don't let that stop you!",None of the above,There are some of the above...
poly8,kelly,Which of the following is NOT true regarding moving average and loess regression,FALSE,They work well both in the middle of the data distribution and at the edges,"For a given $x$, the input values for a moving average are weighted equally, while loess gives greater weight to input values with a predictor value closer to x","Actually, this statement is true! While moving average and loess can include different amounts of the total data when trying to calculate a predicted $Y$ for a given $x$, loess typically gives greater weight to input values whose $x$-value is closer to the desired $x$",They work well both in the middle of the data distribution and at the edges,"Correct; this is not true! Both methods involve predicting Y for a given x by averaging the Y values for data points with x's close to the given x. At the edges of the distirbution, however, there are no data points further out to include in the average, so the predicted Y is unduly influenced by the data points that ARE present closer to the center of the distribution","Both are nonparametric regressions, allowing for nonlinear and distribution specific prediction of $Y$","This is actually true! Both have reduced power to detect monotonic or linear relationships relative to regular parametric regression (like linear regression), but are better able to represent complex or nonlinear relationships",They are typically applied for predictors and outcomes that are both continuous,"This is actually true! Both methods work by averaging the Y values for data points whose x values are close to the given x we want to predict. This requires that the Y values can be averaged (so must be continuous) and that x-values can be defined as ""close to each other"", which typically means continuous (although can apply for certain ordinal categories but not usually)",None of the above,There are some of the above...
transform1,kelly,"You've applied a $log(y)$ transformation to the outcome and found a beta coefficient. If you increase x by one, what will the new predicted $y$ be? $y_0$ = previous predicted $y$",FALSE,$y_0*e^{\beta}$,$y_0 * 100e^{\beta - 1}$,"You're close! The $100e^{\beta} - 1$ is used to give the percent increase in $y$, so if $\beta= 0.405$ then $100 (e^{0.405} -1) = 50$, representing a 50% increase! However, $y_0*50$ is 50 times the $y_0$ value, not a 50% percent increase",$y_0*\beta$,"This beta was calculated based on $log(y)$ so, to find its meaning in terms of y, you have to exponentiate the beta just like you exponentiate the y",$y_0*e^{\beta}$,"that's right! $\beta$ was calculated using the $log(y)$ so, to apply it to y on the non-log scale, you have to exponentiate $\beta$ just like you exponentiate $y$",$y_0 + \beta$,"This would work on a linear scale, but beta was calculated to reduce SSE on the LOG scale of y, so you can't apply this beta directly back to the untransformed y",None of the above,There are some of the above...
transform2,kelly,What is the value of a Box-Cox transformation over a plain power transformation?,FALSE,The Box-Cox transformation puts the SSEs on the same scale for easy comparison when trying multiple different powers,The Box-Cox transformation puts the SSEs on the same scale for easy comparison when trying multiple different powers,"That's right! If we want to find the best power transformation for an outcome, plain power transformations will cause the scale of the SSE to change between different powers, making it ihard to compare. Box-cox can tests different powers while keeping SSE on the same scale, so you can compare which of the tested powers minimized the SSE",They are equivalent,"Um, the equations aren't the same, and box-cox is there for a reason. bitch.","A box-cox power transformation won't tolerate a power transformation to the power of 0, as all $y$'s will become 1","This is actually a weakness of the plain power transformation. In box-cox, if the power tested is 0, then we employ a $log(y)$ transformation instead of the regular equation",All of the above,Incorrect,None of the above,There are some of the above...
select1,kelly,"You are comparing candidate models to a null model. Of the following 4 metrics for comparing two models, which is NOT paired with a correct limitation?",FALSE,F-test: can't test candidates against null models if neither has an intercept,F-test: can't test candidates against null models if neither has an intercept,"Yeah this one is wrong. F-tests can compare any two models as long as one is nested in the other, so the SSE of the larger model is entirely contained within the greater SSE of the smaller model. We often compare a more complete model to the intercept-only model to see if it functions better at explaining the data but, if there is no intercept, then the smaller model can just be the total null model ($y$ is average $y$ value)","$R^2$: always decreases if a predictor is removed, unless using adjusted form","Sorry, hunty, but this is true. The adjusted-$R^2$ uses the same base formula but multiplies the unadjusted-$R^2$ by a ratio that favors a large $n$ but penalizes adding new predictors",SBC: places a larger penalty on large models than AIC,"Sorry but this is a true statement about the SBC. A high AIC or SBC indicates a poor model. Both use the same term to inflate their score if SSE is too high $[n*log(SSE/n)]$ but use different penalty terms relating to adding predictors [p]. AIC adds $2p$ to its score while SBC uses $log(n)p$ and, because $log(n) > 2$ in almost all cases, SBC does penalize addition of predictors more.","Mallows $Cp$: is used to compare a candidate model to a full model, not a null model","This is correct about Mallows $Cp$, which compares the SSE(smaller candidate model) against the MSE(full model) with some other terms relating to n and p, it is not really meaningful if using a null model because it is designed to test the quality of the smaller model, which would be your null model",None of the above,There are some of the above...
select2,kelly,Which of the following is an advantage of data-splitting and generating a model based on the training dataset?,FALSE,It reduces risk of overfitting,It maximizes sample size for model creation,teehee you split your data,The model created does not depend on the exact datasplit,Random data-splitting can cause major changes to the generated model by random chance alone,It reduces risk of overfitting,"By generating your model on a smaller subset of your data, you maintain unmodeled data to test your generalizability against",All of the above,Try again,None of the above,There are some of the above...
select3,kelly,Which of the following approaches to internal data validation is a combination of the methods and advantages/disadvantages of the other two?,TRUE,$k$-fold cross-validation,Leave-out-one cross-validation,"This is a pretty intense approach, in which SSE is generated by comparing each observation to a predicted value made from a model generated WITHOUT that ONE observation",Single-splitting,"In this course, this refers to splitting the dataset into a training dataset and a test set",$k$-fold cross-validation,"Correct! $k$-fold functions like data-splitting, training on training datasets and comparing to test sets. Howver, you make $k$ partitions in the data and, after jackknifing out the data in that partition and creating a model on everything else, you compare the error of the data in the partition (the test set) to the model generated without the data in the partition (everything else is the training set), sorta like leave-out-one corss-validation but for a group of observations rather than just one",All of the above,Incorrect,None of the above,There are some of the above...