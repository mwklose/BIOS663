ID,author,Topic,Question,Ordered,Correct,Ans1,Msg1,Ans2,Msg2,Ans3,Msg3,Ans4,Msg4,Ans5,Msg5
poly1,mwk+tim,poly,What is the maximum number order that can be fit in Polynomial regression?,FALSE,$n - 1$,$n$,"Remember that two points make a line, three points define a parabola, and so forth",$n - 1$,Woohoo!,$p$,We have not introduced the concept of predictors in this question,$p - 1$,We have not introduced the concept of predictors in this question,$1$,"Parabolas exist, as do cubic functions, etc"
poly2,mwk+tim,poly,Which of the following is true about orthogonal polynomials?,FALSE,They are linear combinations of original data.;They are mean zero (except for the intercept);They are mutually ortogonal (uncorrelated),They are linear combinations of original data.,Woohoo!,They do not contain the same information as the original variables.,Orthogonal coding works because it DOES incorporate the same information,The are mean zero (including the intercept),Not including the intercept :),They are mean zero (except for the intercept),Intercept does not have to be zero!!!,They are mutually ortogonal (uncorrelated),This is true!
poly3,mwk+tim,poly,"In orthogonal polynomial coding, what does each variable represent?",FALSE,"$z_1$ captures all information in $x$ beyond the intercept, $z_2$ captures all information in $x^2$ beyond the linear term and intercept, and so forth","$z_1$ captures all information in $x$ beyond the intercept, $z_2$ captures all information in $x^2$ beyond the intercept, and so forth",Reread this carefully :),"$z_1$ captures all information in $x$ beyond the intercept, $z_2$ captures all information in $x^2$ beyond the linear term and intercept, and so forth",Correct!,"$z_1$ captures all information in $x_1$ beyond the intercept, $z_2$ captures all information in $x_2$ beyond the intercept, and so forth","These are subscripts, which then makes this traditional regression","$z_1, z_2$ are completely unrelated to input matrix $x$.",This is wrong. Ooof.,None of the above,There are some of the above...
poly4,kelly,poly,"You have one predictor, one outcome, and 6 data points. What is the minimum order of polynomial would perfectly fit this data, and is it appropriate to use that order in your model?",FALSE,"5, but it would be inappropriate unless you can justify and interpret the use of the higher order polynomial","6, and it would be appropriate because the model will hit all 6 data points with no residuals","It only takes 5 orders to hit all 6 points (remember that a parabola from a second order polynomial can hit three points, and a perfect fit of the model doesn't mean it represents the actual relationship well)","6, but it would be inappropriate unless you can justify and interpret the use of the higher order polynomial",We do not need a 6th order polynomial to perfectly model 6 data points,"5, and it would be appropriate because the model will hit all 6 data points with no residuals",a perfect fit of the model doesn't mean it represents the actual relationship well. How would you interpret a 5th order polynomial for a simple predictor-outcome relationship?,"5, but it would be inappropriate unless you can justify and interpret the use of the higher order polynomial",Heck yeah babyyyyy,None of the above,There are some of the above...
poly5,kelly,poly,"when adding additional orders to a natural polynomial, which of the following is a concern for your model fit?",FALSE,inflation of variance due to collinearity between terms that are based on the same input predictor,inflation of variance due to collinearity between terms that are based on the same input predictor,"Yep! If two terms in the model are derived from the same input data, they will liekly have high collinearity and variance inflation",Inability to capture nonlinear relationships between a predictor and outcome,The purpose of adding additional orders to a polynomial is to allow our model to deviate from a linear relationshio,Adding orders to a polynomial are hard to justify or interpret,"As a rule, adding anything above a cubic polynomial needs to be justified and can be hard to interpret, but lower orders can be added to represent a nonlinear relationship between a predictor and the outcome",All of the above,Incorrect,None of the above,There are some of the above...
poly6,kelly,poly,"in an orthogonal polynomial model with coefficients estimates $a_0$ to $a_k$, performing an F test on paramter $a_3$ is equivalent to what in a natural polynomial model with coefficient estimates $\beta_0$ to $\beta_k$?",FALSE,An in-order test for $\beta_3$,An added-last test for $\beta_3$,Orthogonal polynomials contain only the information added by the additional order of the polynomial and nothing after. An added last test would also be testing against the higher order polynomials in the model in addition to the earlier ones,An in-order test for $\beta_3$,"Slay queen! orthogonal polynomials should contain all additional information added by that order of the natura polynomial, none of the earlier redundant information, and none of the information that comes after",It depends on whether the F test of $a_3$ is added in-order or added-last,"orthogonal polynomials have no collinearity between the input data for each term, so in-order and added-last tests will be equivalent","Nothing, as they are different models","Orthogonal and natural polynomial models are made from the same data, just arranged differently",None of the above,There are some of the above...
poly7,kelly,poly,"After graphing a predictor against an outcome, you see the relationship between the two is linear (and not flat) but changes directions starkly at 4 predictor thresholds. What is an appropriate response?",FALSE,Set linear splines at the thresholds,"Fit a normal linear regression, averaging the relationship across the distribution","This could be justified, but the final regression won't reflect the changing relationships across the predictor distribution",Set linear splines at the thresholds,Yes! These splines will allow the linear relationship to change at these thresholds,Add multiple orders of natural polynomials,"This could work in some cases, but would make the line deviate from linearity between the thresholds. Also, fitting the breakpoints with natural polynomials could require higher than 3 or 4 orders, which is hard to interpret",Give up,"Slay queen! The world is too complex to be meaningfully modeled and your very existence is just a blip in the background chemical chaos of the universe! This answer is still technically incorrect, but don't let that stop you!",None of the above,There are some of the above...
poly8,kelly,poly,Which of the following is NOT true regarding moving average and loess regression,FALSE,They work well both in the middle of the data distribution and at the edges,"For a given $x$, the input values for a moving average are weighted equally, while loess gives greater weight to input values with a predictor value closer to x","Actually, this statement is true! While moving average and loess can include different amounts of the total data when trying to calculate a predicted $Y$ for a given $x$, loess typically gives greater weight to input values whose $x$-value is closer to the desired $x$",They work well both in the middle of the data distribution and at the edges,"Correct; this is not true! Both methods involve predicting Y for a given x by averaging the Y values for data points with x's close to the given x. At the edges of the distirbution, however, there are no data points further out to include in the average, so the predicted Y is unduly influenced by the data points that ARE present closer to the center of the distribution","Both are nonparametric regressions, allowing for nonlinear and distribution specific prediction of $Y$","This is actually true! Both have reduced power to detect monotonic or linear relationships relative to regular parametric regression (like linear regression), but are better able to represent complex or nonlinear relationships",They are typically applied for predictors and outcomes that are both continuous,"This is actually true! Both methods work by averaging the Y values for data points whose x values are close to the given x we want to predict. This requires that the Y values can be averaged (so must be continuous) and that x-values can be defined as ""close to each other"", which typically means continuous (although can apply for certain ordinal categories but not usually)",None of the above,There are some of the above...
poly9,qoua,poly,"You have 1000 of x and y pairs, you create an essence matrix that is 92 x 1. What order polynomial will past through all the points with a $\sigma^2 = 0$?",FALSE,91,1000,Some points are redundant,999,The essence matrix shows what points are redundant,92,"It takes 2 points to fit a straight line, 3 points to fit a parabola",91,Correct,90,One more point is needed
poly10,mwk,poly,A moving average nonparametric smoother,TRUE,Is problematic at the upper and lower limits of $X$,Is problematic at the upper and lower limits of $X$,"This is true, but is it the only thing that is true",Is not sensitive to interval width,It is sensitive to interval width - depends if you use 3 or 5 or 7 or etc.,Work best with one categorical predictor and one continuous predictor,"Lecture 11, Slide 38",Both I and II,II is not true,Both I and III,III is not true
poly11,mwk,poly,The superior nonparametric smoother is,FALSE,"Loess, because it is gives most weight to points close to $X=x$, i.e. locally weighted least squares","Moving average, because of its computational simplicity",Moving average has issues at boundaries,"Loess, because it minimizes finds the local curve with minimized $\beta$ values","I made this up, it focuses on weighted least squares","Loess, because it is the least squares fit for all values within a suitable interval","I mean, this could be true but there is a better answer","Loess, because it is gives most weight to points close to $X=x$, i.e. locally weighted least squares",This is correct.,"Loess, for a combination of two reasons above","Not really, but I understand if this was confusing"
transform1,kelly,transform,"You've applied a $log(y)$ transformation to the outcome and found a beta coefficient. If you increase x by one, what will the new predicted $y$ be? $y_0$ = previous predicted $y$",FALSE,$y_0*e^{\beta}$,$y_0 * 100e^{\beta - 1}$,"You're close! The $100e^{\beta} - 1$ is used to give the percent increase in $y$, so if $\beta= 0.405$ then $100 (e^{0.405} -1) = 50$, representing a 50% increase! However, $y_0*50$ is 50 times the $y_0$ value, not a 50% percent increase",$y_0*\beta$,"This beta was calculated based on $log(y)$ so, to find its meaning in terms of y, you have to exponentiate the beta just like you exponentiate the y",$y_0*e^{\beta}$,"that's right! $\beta$ was calculated using the $log(y)$ so, to apply it to y on the non-log scale, you have to exponentiate $\beta$ just like you exponentiate $y$",$y_0 + \beta$,"This would work on a linear scale, but beta was calculated to reduce SSE on the LOG scale of y, so you can't apply this beta directly back to the untransformed y",None of the above,There are some of the above...
transform2,kelly,transform,What is the value of a Box-Cox transformation over a plain power transformation?,FALSE,The Box-Cox transformation puts the SSEs on the same scale for easy comparison when trying multiple different powers,The Box-Cox transformation puts the SSEs on the same scale for easy comparison when trying multiple different powers,"That's right! If we want to find the best power transformation for an outcome, plain power transformations will cause the scale of the SSE to change between different powers, making it ihard to compare. Box-cox can tests different powers while keeping SSE on the same scale, so you can compare which of the tested powers minimized the SSE",They are equivalent,"Um, the equations aren't the same, and box-cox is there for a reason. bitch.","A box-cox power transformation won't tolerate a power transformation to the power of 0, as all $y$'s will become 1","This is actually a weakness of the plain power transformation. In box-cox, if the power tested is 0, then we employ a $log(y)$ transformation instead of the regular equation",All of the above,Incorrect,None of the above,There are some of the above...
transform3,mwk,transform,For transformations in general:,FALSE,"Choosing certain transformation can make the model difficult to interpret;Zeros or negative numbers mess things up, like with reciprocal and log transformations of pharmaceutical assay measurements;should not back-transform, because $E[f(X)] \neq f(E[X])$","Parallel transformations are not encouraged (i.e. if I take the log of a ratio variable, I should not take the log of all other ratio variables in the model)","Lecture 12, Slide 24",Choosing certain transformation can make the model difficult to interpret,Correct,"Zeros or negative numbers mess things up, like with reciprocal and log transformations of pharmaceutical assay measurements","Correct - pharmaceuticals can have true zeros, whereas blood and urine are usually censored as 0.","should not back-transform, because $E[f(X)] \neq f(E[X])$",Correct - L12 S26,"should back transform, because Jensen's inequality shows us we can","Jensen's Inequality is $E[Y^2] > (E[Y])^2$, which proves the point on why we should not transform"
transform4,mwk,transform,Select all following true statements:,FALSE,Box-Tidwell adds the predictor $xlog(x)$ because of a Taylor Series Expansion;Box-Cox transformation is about selecting a response trasnformation that minimizes the SSE,"Box-Tidwell is about transforming responses, whereas Box-Cox is about transforming predictors",Incorrect - flip the options,Box-Tidwell adds the predictor $xlog(x)$ because of a Taylor Series Expansion,Correct - L12 S28,The Box-Tidwell transform predictor is $\hat \lambda = \frac{\hat \gamma}{\hat \beta}$,L12 S27,"The Box-Tidwell transform predictor is $\hat \lambda = \frac{\hat \gamma}{\hat \beta} + 1$, where $\hat \beta$ comes from the same model that we just fit","Incorrect, L12 S27",Box-Cox transformation is about selecting a response trasnformation that minimizes the SSE,L12 S7 - want SSE
transform5,mwk,transform,What predictor transformation would you suggest for the following graph? ![](https://github.com/mwklose/BIOS663/blob/main/images/q1_sd?raw=true){width=75%},FALSE,There is no transformation that could perfectly adjust the variance,No transformation needed OR not able to tell,"The residuals seem to have a trumpet shape, suggesting the variance is dependent on X value",There is no transformation that could perfectly adjust the variance,"I think this is correct, the variance was constructed as $\theta^2 = abs(x)$",$y^2 \sim x$,Incorrect - the residuals would have a parabolic shape in that case,$log(y) \sim x$,Incorrect - this would have a logarithmic shape,$\sqrt{y} \sim x$,Incorrect - would have square root plot shape
transform6,mwk,transform,What predictor transformation would you suggest for the following graph? ![](https://github.com/mwklose/BIOS663/blob/main/images/q2_para?raw=true){width=75%},FALSE,$y^2 \sim x$,No transformation needed OR not able to tell,You should be able to tell this one. I will let you know if its difficult to see.,No transformation possible to correct this,I generated the data with a simple transformation,$y^2 \sim x$,Correct,$log(y) \sim x$,Incorrect - this would have a logarithmic shape,$y^{3/2} \sim x$,Incorrect - this value cannot be negative
transform7,mwk,transform,What predictor transformation would you suggest for the following graph? ![](https://github.com/mwklose/BIOS663/blob/main/images/q3_log?raw=true){width=75%},FALSE,Some logarithmic transformation,No transformation needed OR not able to tell,The downward spike of residuals at 0 means some sort of domain at 0,No transformation possible to correct this,I generated the data with a transformation,Some transformation that includes a square root,Incorrect - downward spike at 0 suggests some domain issue,Some logarithmic transformation,Correct - this was generated as $log(y) \sim x$,Some transformation involving a reciprocal,Incorrect - that has a different unique shape
transform8,mwk,transform,What predictor transformation would you suggest for the following graph? ![](https://github.com/mwklose/BIOS663/blob/main/images/q4_cub?raw=true){width=75%},FALSE,$y^3 \sim x$,No transformation needed OR not able to tell,Incorrect - you should be able to see the issue,No transformation possible to correct this,I generated the data with a simple transformation,$y^3 \sim x$,Correct - you can see the shape,$y^{3/2} \sim x$,Incorrect - this has a different shape,$y^2 \sim x$,The residuals would have a parabolic shape
transform9,mwk,transform,What predictor transformation would you suggest for the following graph? ![](https://github.com/mwklose/BIOS663/blob/main/images/q5_sr?raw=true){width=75%},FALSE,$y^{1/2} \sim x$,No transformation needed OR not able to tell,Incorrect - no homogeneity,No transformation possible to correct this,"Since this seems to have a somewhat smooth shape, there should be an answer",$y^2 \sim x$,"Incorrect - if I'm not mistaken, this should always open upwards",$y^{1/2} \sim x$,Correct - this was generated with $\sqrt{y+10} \sim x$,$y^3 \sim x$,"Incorrect - although it looks like it could be a cubic region, it is not"
transform10,mwk,transform,What predictor transformation would you suggest for the following graph? ![](https://github.com/mwklose/BIOS663/blob/main/images/q6_pois?raw=true){width=75%},FALSE,$y^{1/2} \sim x$,No transformation needed OR not able to tell,The variance has a trumpet shape,No transformation possible to correct this,I generated this information using a poisson distribution,$y^{1/2} \sim x$,This data was generated from a Poisson distribution,$log(y) \sim x$,Log transformations are better for Gamma distributions,$y^2 \sim x$,The shape is incorrect for this
transform11,mwk,transform,What predictor transformation would you suggest for the following graph? ![](https://github.com/mwklose/BIOS663/blob/main/images/q7_fl?raw=true){width=75%},FALSE,No transformation needed OR not able to tell,No transformation needed OR not able to tell,"The data were simulated by taking the floor and adding random error, so it still should satisfy most assumptions",No transformation possible to correct this,"In this case, is transformation needed",$log(\sqrt{y}) \sim x$,"No, I'm not that cruel",$y \sim xlog(x)$,Not in this case,$sin(y) \sim x$,"No, I'm not this cruel. Plus, this has a cyclic shape"
transform12,mwk,transform,What predictor transformation would you suggest for the following graph? ![](https://github.com/mwklose/BIOS663/blob/main/images/q8_ceil?raw=true){width=75%},FALSE,No transformation needed OR not able to tell,No transformation needed OR not able to tell,"The data were simulated by taking the ceiling and adding random error, so it still should satisfy most assumptions",No transformation possible to correct this,"In this case, is transformation needed",$log(\sqrt{y}) \sim x$,"No, I'm not that cruel",$y \sim xlog(x)$,Not in this case,$cos(y) \sim x$,"No, I'm not this cruel. Plus, this has a cyclic shape"
transform13,mwk,transform,What predictor transformation would you suggest for the following graph? ![](https://github.com/mwklose/BIOS663/blob/main/images/q9_gam?raw=true){width=75%},FALSE,$log(y) \sim x$,No transformation needed OR not able to tell,The variance has a trumpet shape,No transformation possible to correct this,I generated this information using a gamma distribution,$y^{1/2} \sim x$,"Although it does trumpet, it seems to trumpet exponentially at the end",$log(y) \sim x$,Correct - I generated this with a Gamma distribution,$y \sim x$,Incorrect - the trumpet shape does not mean a linear relationship
transform14,mwk,transform,What predictor transformation would you suggest for the following graph? ![](https://github.com/mwklose/BIOS663/blob/main/images/q10_neg2?raw=true){width=75%},FALSE,No transformation needed OR not able to tell,No transformation needed OR not able to tell,"This was generated using $y^{-2} \sim x$, but the residuals are not easy to see a difference",No transformation possible to correct this,"This is an acceptable answer, it's difficult to see exactly what was done",$y^{1/2} \sim x$,Nope - this shape somewhat trumpets,$log(y) \sim x$,"There is not a spike close to zero, which can be difficult to see",$cos(y) \sim x$,"No, I'm not this cruel. Plus, this has a cyclic shape"
select1,kelly,select,"You are comparing candidate models to a null model. Of the following 4 metrics for comparing two models, which is NOT paired with a correct limitation?",FALSE,F-test: can't test candidates against null models if neither has an intercept,F-test: can't test candidates against null models if neither has an intercept,"Yeah this one is wrong. F-tests can compare any two models as long as one is nested in the other, so the SSE of the larger model is entirely contained within the greater SSE of the smaller model. We often compare a more complete model to the intercept-only model to see if it functions better at explaining the data but, if there is no intercept, then the smaller model can just be the total null model ($y$ is average $y$ value)","$R^2$: always decreases if a predictor is removed, unless using adjusted form","Sorry, hunty, but this is true. The adjusted-$R^2$ uses the same base formula but multiplies the unadjusted-$R^2$ by a ratio that favors a large $n$ but penalizes adding new predictors",SBC: places a larger penalty on large models than AIC,"Sorry but this is a true statement about the SBC. A high AIC or SBC indicates a poor model. Both use the same term to inflate their score if SSE is too high $[n*log(SSE/n)]$ but use different penalty terms relating to adding predictors [p]. AIC adds $2p$ to its score while SBC uses $log(n)p$ and, because $log(n) > 2$ in almost all cases, SBC does penalize addition of predictors more.","Mallows $Cp$: is used to compare a candidate model to a full model, not a null model","This is correct about Mallows $Cp$, which compares the SSE(smaller candidate model) against the MSE(full model) with some other terms relating to n and p, it is not really meaningful if using a null model because it is designed to test the quality of the smaller model, which would be your null model",None of the above,There are some of the above...
select2,kelly,select,Which of the following is an advantage of data-splitting and generating a model based on the training dataset?,FALSE,It reduces risk of overfitting,It maximizes sample size for model creation,teehee you split your data,The model created does not depend on the exact datasplit,Random data-splitting can cause major changes to the generated model by random chance alone,It reduces risk of overfitting,"By generating your model on a smaller subset of your data, you maintain unmodeled data to test your generalizability against",All of the above,Try again,None of the above,There are some of the above...
select3,kelly,select,Which of the following approaches to internal data validation is a combination of the methods and advantages/disadvantages of the other two?,TRUE,$k$-fold cross-validation,Leave-out-one cross-validation,"This is a pretty intense approach, in which SSE is generated by comparing each observation to a predicted value made from a model generated WITHOUT that ONE observation",Single-splitting,"In this course, this refers to splitting the dataset into a training dataset and a test set",$k$-fold cross-validation,"Correct! $k$-fold functions like data-splitting, training on training datasets and comparing to test sets. Howver, you make $k$ partitions in the data and, after jackknifing out the data in that partition and creating a model on everything else, you compare the error of the data in the partition (the test set) to the model generated without the data in the partition (everything else is the training set), sorta like leave-out-one corss-validation but for a group of observations rather than just one",All of the above,Incorrect,None of the above,There are some of the above...
code1,kelly,code,Which of the following is not true about classical ANOVA modeling,FALSE,The mean of group 1 $\mu_1 = \beta_1$,The mean of group 1 $\mu_1 = \beta_1$,"Classical ANOVA modeling uses an unmeasured $\beta_0$ representing the population mean, and then creates $\beta_k$ to be the difference between this $\beta_0$ and the k group mean. $\mu_k = \beta_0 + \beta_1$ and, because $\beta_0$ is chosen arbirtrarily, none of these $\beta$s have intrinsic meaning taken out of context of each other ($\beta_1$ has no meaning by itself), also meaning that the X matrix is not full rank because it has a redundant column.",The beta coefficients are not estimable,"This is acutally true. Classical anova modeling posits that $\mu_k = \beta_0 + \beta_{k}$ and there is one $\beta_k$ for each group as well as a $\beta_0$ to represent the theoretical grand mean of the whole population. However, we don't measure this grand mean, so $\beta_0$ can really be set at any value and all the other $\beta$ coefficients could be adjusted to make sure their addition causes the sum for group k to reach $\mu_k$ and the model would still work. As such, the beta coefficients in classical ANOVA coding do not have inherent interpretability on their own.",The X matrix is not full rank,"This is actually true! The X matrix contains redudnant information in being all 0's and 1's with a unique indicator variable for each group and also an intercept. The other ANOVA models are able to contain the exact same categorical information in an X matrix using one less predictor (such as in the reference cell model, where you have an intercept and k-1 indicator variables and the reference group just has 0's for all the indicators instead of having its own separate variable), which makes them full rank.","The grand mean in a classical ANOVA model is a hypothetical, unmeasured value, whereas the grand mean in effect model coding is the actual mean of the recorded outcome variable across the dataset.",Sorry queen but this is right,None of the above,There are some of the above...
anova1,kelly,anova,you are testing for homogeneity of variance between 3 treatment groups in your ANOVA. Which is true?,FALSE,A significant bartlett's test indicates heteroscedasticity or just non-normality of the data,Hartley's test examines whether any one group has a different variance from any other,Hartley's checks whether the maximum variance among the groups is significantly different from the minimum variance of the groups,"If homogeneity of variance is rejected, then Welch's ANOVA can't be used",Welch's ANOVA is specifically employed as a version of ANOVA that is still robust when we don't have homogeneity of variance,A significant bartlett's test indicates heteroscedasticity or just non-normality of the data,"Right! A bartlett's test asks tests whether the variance (of errors around each group mean) of at least one group is different from the variance of another. However, the bartlett test can also be significant of one of the groups is not normally distributed",All of the above,Incorrect,None of the above,There are some of the above...
ancova1,qoua,ancova,ANCOVA requires how many interactions in a model with 2 categorical variables (with 3 levels in each) and 2 continuous variables?,FALSE,0,1,Review Lec 17 Slide 2,2,Review Lec 17 Slide 2,3,Review Lec 17 Slide 2,4,Review Lec 17 Slide 2,None of the above,There are some of the above...
ancova1,qoua,ancova,"How are ANCOVA models different from the ""Full in every cell models?""",FALSE,The slope of the interactions variables betweeen the continous and categorical variable are the same.,The slope of the interactions variables betweeen the continous and categorical variable are the same.,This is what makes ANCOVA a special case of full model in every cell,The are the same.,Review Lec 17 Slide 2 - they are almost the same but not exactly,"ANCOVA model has interactions terms and ""Full model in every cell models"" do not.",Review Lec 17 Slide 2,All of the above,Review Lec 17 Slide 2,None of the above,There are some of the above...
code2,qoua,code,"In reference coding, if you have a categorical variable that has 33 levels, how many j columns will you have in your design matrix?",FALSE,33,33,This includes the intercept,32,What happen to the intercept?,31,Nope - what items would you be dropping?,2,"No, focus on levels",34,Incorrect
log1,natalie,logistic_regression,I want to test the effect of drug X on condition Z by dose Y. Which of the following should be done to test signficance of interaction effect?,FALSE,A likelihood ratio test of model with interaction terms and model with only fixed effects,Pearson's goodness of fit,nah,A Wald test with all parametes equal to zero,"This in theory could work, but there is a superior method",Hosmer-Lemeshow,nah,A likelihood ratio test of model with interaction terms and model with only fixed effects,LRT is best approach when working with nested models,No way to test,Incorrect
log2,natalie,logistic_regression,"Throwback to the penguin example! We're worried that our flippered friends are becoming underweight. We modeled the expected value of underweight status (no/yes) as a logit function of: species, sex, and island. We get an intercept value of .43. What does this mean?",FALSE,ln (odds) of being underweight,the probabilty of being underweight,"nah, but related",the odds of females being underweight,nope,ln (odds) of being underweight,"In logistic regression, we must log transform the odds to create a linear relationship of the outcome with the predictor variables.",the mean weight in kilograms,I hope not because that would be an emaciated penguin,More information needed,Incorrect
md1,natalie,missing_data,A clinical trial scheuled to run for 24 months recruits people from a 65 mile radius and requires in clinic visits every month. All participants who live more than 30 miles from the clinic drop out of the study by 6 months. What type of missing data is observed in this situation?,FALSE,MNAR,MCAR,nah,MAR,nah,MNAR,The missingness is related to a factor that is not measured by the researcher (distance from study site),Data are not missing,nah,MICE,Incorrect